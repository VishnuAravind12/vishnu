{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Variable Linear Regression\n",
    "\n",
    "If you're embarking on a project that requires predicting outcomes based on several input variables, **multi-variable linear regression** is an important technique. This approach builds upon simple linear regression, which predicts a dependent variable using a single independent variable, by incorporating multiple independent variables.\n",
    "\n",
    "## Introduction to Multi-Variable Linear Regression\n",
    "\n",
    "**Multi-variable linear regression**, also known as **multiple linear regression** or **multivariate linear regression**, is a statistical method used to model the relationship between a dependent variable and two or more independent variables. The goal is to predict the dependent variable's value based on the independent variables.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "The formula for a multi-variable linear regression with `n` independent variables is:\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + ... + bnxn + ε\n",
    "\n",
    "- `y` is the predicted value of the dependent variable.\n",
    "- `x1, x2, ..., xn` are the independent variables.\n",
    "- `b0` is the y-intercept or the prediction when all x's are zero.\n",
    "- `b1, b2, ..., bn` are the coefficients, representing the influence of each independent variable on the prediction.\n",
    "- `ε` represents the error term, which is the part of `y` that isn't explained by the x variables.\n",
    "\n",
    "## The Mathematics Behind It\n",
    "\n",
    "To determine the optimal coefficients (`b0, b1, ..., bn`) that minimize prediction errors, we use a cost function, typically the **Mean Squared Error (MSE)**. The MSE calculates the average squared difference between the actual and predicted values.\n",
    "\n",
    "Optimization is usually achieved through **Gradient Descent** or by solving the **Normal Equation**. Gradient Descent adjusts the coefficients iteratively to minimize the MSE, while the Normal Equation computes the coefficients directly through matrix operations.\n",
    "\n",
    "## Optimizing the Coefficients\n",
    "\n",
    "In **Gradient Descent**, coefficients are updated using the rule:\n",
    "\n",
    "bi := bi - α ∂/∂bi MSE\n",
    "\n",
    "- `bi` is the coefficient's current value.\n",
    "- `α` is the learning rate, controlling the step size towards the minimum MSE.\n",
    "- `∂/∂bi MSE` is the partial derivative of the MSE with respect to `bi`.\n",
    "\n",
    "The learning rate is a hyperparameter that needs to be chosen with care to ensure proper convergence.\n",
    "\n",
    "With the **Normal Equation**, coefficients are calculated as:\n",
    "\n",
    "b = (XTX)^−1*XTy\n",
    "\n",
    "- `X` is the matrix of input features.\n",
    "- `XT` is the transpose of X.\n",
    "- `y` is the vector of observed values for the dependent variable.\n",
    "- `(XTX)^−1XTy` is the matrix operation yielding the coefficient vector.\n",
    "\n",
    " It's a closed-form solution, meaning it gives us the exact coefficients in one calculation, as opposed to iterative methods like Gradient Descent, which approach the solution step by step. The Normal Equation method is straightforward but can be computationally intensive for large datasets or when `(XTX)` is not invertible.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "**Multi-variable linear regression** is a potent analytical tool, enabling the modeling of complex relationships between variables. By optimizing coefficients, it allows for nuanced predictions and insights into how each independent variable influences the dependent variable. This technique is widely applicable across various domains, including finance, healthcare, and education, providing a data-driven foundation for decision-making and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "%jars /home/vishnuaa77/vscode/vishnu/lib/commons-math3-3.6.1.jar\n",
    "%jars /home/vishnuaa77/vscode/vishnu/lib/jfreechart-1.5.4.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-500.9701492539607, -18.805970149260247, -0.5970149253726049, -0.8208955223876728, 29.059701492544264]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.commons.math3.linear.Array2DRowRealMatrix;\n",
    "import org.apache.commons.math3.linear.ArrayRealVector;\n",
    "import org.apache.commons.math3.linear.LUDecomposition;\n",
    "import org.apache.commons.math3.linear.RealMatrix;\n",
    "import org.apache.commons.math3.linear.RealVector;\n",
    "\n",
    "public class BasketballPerformanceRegression {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        // Mock data for 5 basketball players\n",
    "        double[][] xData = {\n",
    "            {22, 7, 10, 35}, // Player 1: points, assists, rebounds, minutes\n",
    "            {15, 12, 8, 30}, // Player 2\n",
    "            {30, 4, 12, 40}, // Player 3\n",
    "            {8,  10, 5, 25}, // Player 4\n",
    "            {27, 3, 7,  38}  // Player 5\n",
    "        };\n",
    "        double[] yData = {90, 75, 85, 65, 88}; // Performance scores for each player\n",
    "\n",
    "        double[] coefficients = calculateCoefficients(xData, yData);\n",
    "        System.out.println(\"Coefficients: \" + java.util.Arrays.toString(coefficients));\n",
    "    }\n",
    "\n",
    "    public static double[] calculateCoefficients(double[][] xData, double[] yData) {\n",
    "        int n = xData.length;\n",
    "        int m = xData[0].length;\n",
    "        RealMatrix X = new Array2DRowRealMatrix(n, m + 1); // +1 for bias term\n",
    "        RealVector Y = new ArrayRealVector(yData, false);\n",
    "\n",
    "        for (int i = 0; i < n; i++) {\n",
    "            X.setEntry(i, 0, 1);  // Bias term\n",
    "            for (int j = 0; j < m; j++) {\n",
    "                X.setEntry(i, j + 1, xData[i][j]);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        RealMatrix Xt = X.transpose();\n",
    "        RealMatrix XtX = Xt.multiply(X);\n",
    "        RealMatrix XtXInverse = new LUDecomposition(XtX).getSolver().getInverse();\n",
    "        RealVector XtY = Xt.operate(Y);\n",
    "        RealVector B = XtXInverse.operate(XtY);\n",
    "\n",
    "        return B.toArray();\n",
    "    }\n",
    "}\n",
    "\n",
    "BasketballPerformanceRegression.main(null);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "17.0.8+7-Ubuntu-120.04.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
